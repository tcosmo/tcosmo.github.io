{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# jdc for step by step class construction\n",
    "import jdc.jdc as jdc\n",
    "\n",
    "import numpy as np\n",
    "import itertools \n",
    "\n",
    "class SOM(object):\n",
    "    def __init__(self,h,w,dim_feat):\n",
    "        \"\"\"\n",
    "            Construction of a zero-filled SOM.\n",
    "            h,w,dim_feat: constructs a (h,w,dim_feat) SOM.\n",
    "        \"\"\"\n",
    "        self.shape = (h,w,dim_feat)\n",
    "        self.som = np.zeros((h,w,dim_feat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%add_to SOM\n",
    "def train(self,data):\n",
    "    \"\"\" \n",
    "        Training procedure for a SOM.\n",
    "        data: a N*d matrix, N the number of examples, \n",
    "              d the same as dim_feat=self.shape[2].\n",
    "    \"\"\"\n",
    "    for t in itertools.count():\n",
    "        i_data =  np.random.choice(range(len(data)))\n",
    "        bmu = self.find_bmu(data[i_data])\n",
    "        \n",
    "def find_bmu(self, input_vec):\n",
    "    \"\"\"\n",
    "        Find the BMU of a given input vector.\n",
    "        input_vec: a d=dim_feat=self.shape[2] input vector.\n",
    "    \"\"\"\n",
    "    list_bmu = []\n",
    "    for y in range(self.shape[0]):\n",
    "        for x in range(self.shape[1]):\n",
    "            dist = np.linalg.norm((input_vec-self.som[y,x]))\n",
    "            list_bmu.append(((y,x),dist))\n",
    "    list_bmu.sort(key=lambda x: x[1])\n",
    "    return list_bmu[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%add_to SOM\n",
    "def __init__(self,h,w,dim_feat):\n",
    "    \"\"\"\n",
    "        Construction of a zero-filled SOM.\n",
    "        h,w,dim_feat: constructs a (h,w,dim_feat) SOM.\n",
    "    \"\"\"\n",
    "    self.shape = (h,w,dim_feat)\n",
    "    self.som = np.zeros((h,w,dim_feat))\n",
    "    \n",
    "    # Training parameters\n",
    "    self.L0 = 0.0\n",
    "    self.lam = 0.0\n",
    "    \n",
    "def train(self,data,L0,lam):\n",
    "    \"\"\" \n",
    "        Training procedure for a SOM.\n",
    "        data: a N*d matrix, N the number of examples, \n",
    "              d the same as dim_feat=self.shape[2].\n",
    "        L0,lam: training parameters.\n",
    "    \"\"\"\n",
    "    self.L0 = L0\n",
    "    self.lam = lam\n",
    "    \n",
    "    for t in itertools.count():\n",
    "        i_data =  np.random.choice(range(len(data)))\n",
    "        \n",
    "        bmu = self.find_bmu(data[i_data])\n",
    "        self.update_bmu(bmu,data[i_data],t)\n",
    "            \n",
    "        \n",
    "def update_bmu(self,bmu,input_vector,t):\n",
    "    \"\"\"\n",
    "        Update rule for the BMU.\n",
    "        bmu: (y,x) BMU's coordinates.\n",
    "        input_vector: current data vector.\n",
    "        t: current time.\n",
    "    \"\"\"\n",
    "    self.som[bmu] += self.L(t)*(input_vector-self.som[bmu])\n",
    "\n",
    "def L(self, t):\n",
    "    \"\"\"\n",
    "        Learning rate formula.\n",
    "        t: current time.\n",
    "    \"\"\"\n",
    "    return self.L0*np.exp(-t/self.lam) \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%add_to SOM\n",
    "def __init__(self,h,w,dim_feat):\n",
    "    \"\"\"\n",
    "        Construction of a zero-filled SOM.\n",
    "        h,w,dim_feat: constructs a (h,w,dim_feat) SOM.\n",
    "    \"\"\"\n",
    "    self.shape = (h,w,dim_feat)\n",
    "    self.som = np.zeros((h,w,dim_feat))\n",
    "    \n",
    "    # Training parameters\n",
    "    self.L0 = 0.0\n",
    "    self.lam = 0.0\n",
    "    self.sigma0 = 0.0\n",
    "    \n",
    "def train(self,data,L0,lam,sigma0):\n",
    "    \"\"\" \n",
    "        Training procedure for a SOM.\n",
    "        data: a N*d matrix, N the number of examples, \n",
    "              d the same as dim_feat=self.shape[2].\n",
    "        L0,lam,sigma0: training parameters.\n",
    "    \"\"\"\n",
    "    self.L0 = L0\n",
    "    self.lam = lam\n",
    "    self.sigma0 = sigma0\n",
    "    \n",
    "    for t in itertools.count():\n",
    "        i_data =  np.random.choice(range(len(data)))\n",
    "        \n",
    "        bmu = self.find_bmu(data[i_data])\n",
    "        self.update_som(bmu,data[i_data],t)\n",
    "\n",
    "            \n",
    "def update_som(self,bmu,input_vector,t):\n",
    "    \"\"\" \n",
    "        Calls the update rule on each cell.\n",
    "        bmu: (y,x) BMU's coordinates.\n",
    "        input_vector: current data vector.\n",
    "        t: current time.\n",
    "    \"\"\"\n",
    "    for y in range(self.shape[0]):\n",
    "        for x in range(self.shape[1]):\n",
    "            dist_to_bmu = np.linalg.norm((np.array(bmu)-np.array((y,x))))\n",
    "            self.update_cell((y,x),dist_to_bmu,input_vector,t)\n",
    "\n",
    "def update_cell(self,cell,dist_to_bmu,input_vector,t):\n",
    "    \"\"\" \n",
    "        Computes the update rule on a cell.\n",
    "        cell: (y,x) cell's coordinates.\n",
    "        dist_to_bmu: L2 distance from cell to bmu.\n",
    "        input_vector: current data vector.\n",
    "        t: current time.\n",
    "    \"\"\"\n",
    "    self.som[cell] += self.N(dist_to_bmu,t)*self.L(t)*(input_vector-self.som[cell])\n",
    "\n",
    "def N(self,dist_to_bmu,t):\n",
    "    \"\"\" \n",
    "        Computes the neighbouring penalty.\n",
    "        dist_to_bmu: L2 distance to bmu.\n",
    "        t: current time.\n",
    "    \"\"\"\n",
    "    curr_sigma = self.sigma(t)\n",
    "    return np.exp(-(dist_to_bmu**2)/(2*curr_sigma**2))\n",
    "\n",
    "def sigma(self, t):\n",
    "    \"\"\"\n",
    "        Neighbouring radius formula.\n",
    "        t: current time.\n",
    "    \"\"\"\n",
    "    return self.sigma0*np.exp(-t/self.lam) \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%add_to SOM\n",
    "def train(self,data,L0,lam,sigma0,initializer=np.random.rand):\n",
    "    \"\"\" \n",
    "        Training procedure for a SOM.\n",
    "        data: a N*d matrix, N the number of examples, \n",
    "              d the same as dim_feat=self.shape[2].\n",
    "        L0,lam,sigma0: training parameters.\n",
    "        initializer: a function taking h,w and dim_feat (*self.shape) as \n",
    "                     parameters and returning an initial (h,w,dim_feat) tensor.\n",
    "    \"\"\"\n",
    "    self.L0 = L0\n",
    "    self.lam = lam\n",
    "    self.sigma0 = sigma0\n",
    "    \n",
    "    self.som = initializer(*self.shape)\n",
    "    \n",
    "    for t in itertools.count():\n",
    "        i_data =  np.random.choice(range(len(data)))\n",
    "        \n",
    "        bmu = self.find_bmu(data[i_data])\n",
    "        self.update_som(bmu,data[i_data],t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%add_to SOM\n",
    "def train(self,data,L0,lam,sigma0,initializer=np.random.rand):\n",
    "    \"\"\" \n",
    "        Training procedure for a SOM.\n",
    "        data: a N*d matrix, N the number of examples, \n",
    "              d the same as dim_feat=self.shape[2].\n",
    "        L0,lam,sigma0: training parameters.\n",
    "        initializer: a function taking h,w and dim_feat (*self.shape) as \n",
    "                     parameters and returning an initial (h,w,dim_feat) tensor.\n",
    "    \"\"\"\n",
    "    self.L0 = L0\n",
    "    self.lam = lam\n",
    "    self.sigma0 = sigma0\n",
    "    \n",
    "    self.som = initializer(*self.shape)\n",
    "    \n",
    "    for t in itertools.count():\n",
    "        if self.sigma(t) < 1.0:\n",
    "            break\n",
    "\n",
    "        i_data =  np.random.choice(range(len(data)))\n",
    "        \n",
    "        bmu = self.find_bmu(data[i_data])\n",
    "        self.update_som(bmu,data[i_data],t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%add_to SOM\n",
    "def __init__(self,h,w,dim_feat):\n",
    "    \"\"\"\n",
    "        Construction of a zero-filled SOM.\n",
    "        h,w,dim_feat: constructs a (h,w,dim_feat) SOM.\n",
    "    \"\"\"\n",
    "    self.shape = (h,w,dim_feat)\n",
    "    self.som = np.zeros((h,w,dim_feat))\n",
    "    \n",
    "    # Training parameters\n",
    "    self.L0 = 0.0\n",
    "    self.lam = 0.0\n",
    "    self.sigma0 = 0.0\n",
    "    \n",
    "    self.data = []\n",
    "\n",
    "def train(self,data,L0,lam,sigma0,initializer=np.random.rand):\n",
    "    \"\"\" \n",
    "        Training procedure for a SOM.\n",
    "        data: a N*d matrix, N the number of examples, \n",
    "              d the same as dim_feat=self.shape[2].\n",
    "        L0,lam,sigma0: training parameters.\n",
    "        initializer: a function taking h,w and dim_feat (*self.shape) as \n",
    "                     parameters and returning an initial (h,w,dim_feat) tensor.\n",
    "    \"\"\"\n",
    "    self.L0 = L0\n",
    "    self.lam = lam\n",
    "    self.sigma0 = sigma0\n",
    "    \n",
    "    self.som = initializer(*self.shape)\n",
    "    \n",
    "    self.data = data\n",
    "    \n",
    "    for t in itertools.count():\n",
    "        if self.sigma(t) < 1.0:\n",
    "            break\n",
    "\n",
    "        i_data =  np.random.choice(range(len(data)))\n",
    "        \n",
    "        bmu = self.find_bmu(data[i_data])\n",
    "        self.update_som(bmu,data[i_data],t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%add_to SOM\n",
    "def quant_err(self):\n",
    "    \"\"\" \n",
    "        Computes the quantization error of the SOM.\n",
    "        It uses the data fed at last training.\n",
    "    \"\"\"\n",
    "    bmu_dists = []\n",
    "    for input_vector in self.data:\n",
    "        bmu = self.find_bmu(input_vector)\n",
    "        bmu_feat = self.som[bmu]\n",
    "        bmu_dists.append(np.linalg.norm(input_vector-bmu_feat))\n",
    "    return np.array(bmu_dists).mean()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
